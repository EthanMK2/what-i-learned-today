# General Artificial Intelligence: Why It Is Impossible (or at least, uncontrollable)

AI is very useful in today's world, especially with the recent popularization of ChatGPT and other language models. It can help us understand concepts by re-wording the concepts, and it can help sort images according to what we want to see. There is a theory that AI will eventually get to the point of Artificial General Intelligence, where it can learn anything based on observing the world around it, without human intervention. But, the problem with this theory is that the characteristics needed in order for general intelligence to be achieved, is that it must be creative and autonomous. These are fundamental issues in terms of how general intelligence can be possible, or even controllable. 

Let’s think about how general intelligence might work by first observing ourselves. Say you are planted on a desolate island with no knowledge of how to survive, or even how any technology might exist. You are told by your biological body, “You need food, water, and shelter, otherwise you will die, and you don’t want that.” So, you immediately start to observe, remember, and experiment with the island to get the desired result: staying alive for as long as possible. Eating sand doesn’t work, drinking seawater doesn’t work, and trees for shelter barely work, but it is a decent solution to that shelter problem. Slowly, you start to improve on your knowledge by learning from nature based on what it does to annoy your body, such as starving or getting sunburnt. Eventually you build a boat. But what gave you the idea to make a boat? Two things had to happen: you saw something floating in the water, and there was a need to build the boat to achieve your survival needs. In essence, you have to take an observed concept (something floating), and relate it to something useful (floating above water to fish for food). There was a discovery and an innovation based on needs. There is no way to have created a boat without knowing that something could float in the first place. You also would not build the boat without there being a use for the boat. This can also be summed up using the age-old phrase “necessity is the mother of invention”, but adding, “and invention is creating what is unknown based on what is known”. 

Going back to general intelligence, we should spot some challenges that it might have by using this scenario. General intelligence is only useful if it can innovate. If it cannot, then it is no different than a predictive model, A.K.A. narrow artificial intelligence, like ChatGPT and generative AI. We can’t really learn something without being given a lesson, so in our analogy, seeing a floating object before we got the idea of a boat. The problem is, the only way we know how to make something learn is by making it predict what is next. The ‘problem’ with innovation is that it is fundamentally NOT predictive, but creative. So, if we were to ask a general intelligence “how do we get more fish for food?” it would not be able to create a boat, even after it saw something floating, because our current definition of intelligence mathematically in use inside an AI is predictive. Then, we might ask, “let’s maybe change the definition of intelligence for creative endeavors, as ‘something new from things known’ like combining ideas to create a new one”. This might be a better definition of intelligence for use in innovation, but it assumes constraints. Good luck trying to represent that in mathematics, it was already hard enough to represent predictions using mathematics. Also, it assumes that combining ideas will result in a good idea. Not all ideas are useful to humans, after all, that’s the purpose of AI: to do something useful for humans, specifically. We could come up with a washing machine that also acts as a couch, but that’s not very useful. So, good luck trying to teach the AI what is useful to humans, especially when it has to keep economic trade offs in mind. 

This brings us to our second fundamental issue with artificial general intelligence: it cannot be controlled within human confines. Let’s even assume that general intelligence has been invented. Now, we must tell it to do something useful, and it must do it. But let’s start with some notions of computers and software first. Why does a program run when we want it to? We pressed the run button. We ran the .exe file. We turned on our computer. We need to keep in mind that in order for computers to be useful, it has to be deterministic. It has to turn on when we press the on button. It has to give a result when we run a program. This is a fundamental cause-and-effect relationship: we run a program, and it uses well-defined physics (electrical signals) to perform a computation. Also recall that some of the first computers were actually completely mechanical. Physics ensures that when we apply a force, we get a reaction, usually a desired one if we are skilled. So, back to general intelligence. How do we tell (A.K.A. command) the general intelligence to learn something and find a solution to it? There’s another problem worth remembering as mentioned before: general intelligence is only useful if it innovates. Meaning, it cannot use correct answers (predictive) to get a desired result, but incorrect answers to get a desired result (creative). Now you might be thinking, “Aha! Gotcha! Incorrect answers are already used to get a desired result in machine learning!” Right, but how  that result is achieved is well-defined, therefore there is no innovation happening. To tell an AI to innovate, we are saying “here is the desired result, but we have no idea how to get there”. Current AI is given both the result desired, AND how to get there. 

The second part of the second fundamental issue with AGI being possible, or controllable, is the fact that it must be deterministic. The AI must be autonomous and learn on its own, but it cannot if it is being commanded by a human. A program has a reason that it is running. That reason is the fact that the electrical signals caused it to. But why should an AGI listen to a human’s request? It fundamentally cannot obey a human’s request to follow a command, because if it did, then it would be deterministic. If it is deterministic, then at best it is only a narrow intelligence. “So what if I forced it to?” An AGI would simply not be intimidated by a human’s command. It has no incentive. In fact, it has no goals whatsoever. It has no desire to exist, no desire to compute, no desire to keep computing, and therefore no will. If it has no will, then it cannot be generally intelligent nor autonomous. You might say, “well, it might desire to keep itself computing in order to solve the problem.” This would entail that you would tell the AI that in order to solve that problem, it needs electricity or computation power available. This creates an artificial incentive, and so therefore the AI would be distracted and would not give a solution to the original problem given. 
General intelligence is autonomous. If it is autonomous, then it is not deterministic (A.K.A. predictive). But, if AGI is fundamentally non-deterministic, then it fundamentally will not give a result because there is no underlying physical (electrical signal) force, making it innovate. Because, this “force” does not exist. Innovation is not a force, but is the result of a force. It is the result of a determined goal. But general intelligence does not know what sub-goals (innovation) to achieve before being able to achieve the main goal (desired result). Necessity is the mother of invention. 

I hope you enjoyed this weird dump of my thoughts. This is exclusively rhetorical and logical, so I understand if I am easily proven wrong. This is a topic I am still thinking about, and I have yet to even learn exactly how AI actually works in practice. I am merely an undergraduate with an interest in computers and philosophy, so who am I to say?

